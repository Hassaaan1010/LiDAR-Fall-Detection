{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassaan/Downloads/deployement/LiDAR-Pose-Detection/deploymentEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hassaan/Downloads/deployement/LiDAR-Pose-Detection/deploymentEnv/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./utils\") \n",
    "from utils.monodepth import load_model, depth_map_from_frame\n",
    "# Load the model\n",
    "dpt_model, transform, device = load_model()\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pose_detection(frame, model):\n",
    "\n",
    "    # Convert the frame to a format suitable for the model\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get the depth map\n",
    "    depth_map = depth_map_from_frame(img_rgb, dpt_model, transform, device)\n",
    "    print(\"depth map formed\")\n",
    "    cv2.imwrite(\"output.jpg\",depth_map)\n",
    "    \n",
    "    depth_map = cv2.imread(\"output.jpg\")    \n",
    "    \n",
    "    num_people = 0\n",
    "    # Perform inference\n",
    "    results = model.predict(depth_map)\n",
    "    \n",
    "    keypoints = results[0].keypoints\n",
    "    \n",
    "    if len(keypoints) > 0:\n",
    "        keypoints = keypoints[0].cpu().numpy()\n",
    "    else:\n",
    "        keypoints = None\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    if keypoints is not None:\n",
    "        xy = keypoints.data[0]\n",
    "        conf = keypoints.conf[0]\n",
    "        for i in range(len(xy)):\n",
    "            x, y, c = xy[i]\n",
    "            if c > 0:\n",
    "                cv2.circle(depth_map, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "                cv2.putText(\n",
    "                    depth_map,\n",
    "                    f\"{i}-{int(c*100)}\",\n",
    "                    (int(x), int(y) - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 0, 0),\n",
    "                    1,\n",
    "                )\n",
    "                \n",
    "    while True:\n",
    "        cv2.imshow(\"C\", depth_map)\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return 123, depth_map\n",
    "    # return num_people, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"models/yolov8m-pose.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 1288.2ms\n",
      "Speed: 4.8ms preprocess, 1288.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Number of detected people: 123\n"
     ]
    }
   ],
   "source": [
    "ret = True\n",
    "frame = cv2.imread(\"/home/hassaan/Downloads/train_2500/images_final/17.png\")\n",
    "\n",
    "\n",
    "if ret:\n",
    "    num_people, processed_frame = apply_pose_detection(frame, model)\n",
    "    print(f\"Number of detected people: {num_people}\")\n",
    "\n",
    "    # # Display the processed frame\n",
    "    # cv2.imshow('Processed Frame', processed_frame)\n",
    "    # cv2.waitKey(0)\n",
    "\n",
    "# cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploymentEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
